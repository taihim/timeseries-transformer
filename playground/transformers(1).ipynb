{
 "cells": [
  {
   "cell_type": "code",
   "id": "2ed092cc-d9f0-40ba-838f-3c96920e8016",
   "metadata": {
    "id": "2ed092cc-d9f0-40ba-838f-3c96920e8016",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:01.013292Z",
     "start_time": "2024-07-09T21:20:58.681508Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "7a96e134-d7c9-4ddc-843a-29b7201e8eac",
   "metadata": {
    "id": "7a96e134-d7c9-4ddc-843a-29b7201e8eac",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.175984Z",
     "start_time": "2024-07-09T21:21:08.361357Z"
    }
   },
   "source": [
    "class FordDataset(Dataset):\n",
    "  def __init__(self, split=\"train\"):\n",
    "    self.root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "    self.data = torch.tensor(np.loadtxt(self.root_url + \"FordA_TRAIN.tsv\", delimiter=\"\\t\"), dtype=torch.float32) if split==\"train\" else torch.tensor(np.loadtxt(self.root_url + \"FordA_TEST.tsv\", delimiter=\"\\t\"), dtype=torch.float32)\n",
    "    self.labels = self.data[:, 0] # get first element from each example\n",
    "    self.sequences = self.data[:, 1:] # get all elements after first element\n",
    "    self.labels[self.labels == -1] = 0 # change all -1 labels to 0\n",
    "    self.num_classes = len(torch.unique(self.labels)) # count the number of unique labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.data.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sequence = torch.reshape(self.sequences[idx], (-1, 1)) # dim: seq_len x num_features\n",
    "    label = torch.reshape(self.labels[idx], (-1, )) # dim: 1 x 1\n",
    "\n",
    "    return sequence, label\n",
    "\n",
    "train_dataset = FordDataset(\"train\")\n",
    "test_dataset = FordDataset(\"test\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "60b6dce8-9c83-4c2a-98cc-f2997f1c4f4e",
   "metadata": {
    "id": "60b6dce8-9c83-4c2a-98cc-f2997f1c4f4e",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.180682Z",
     "start_time": "2024-07-09T21:21:14.177488Z"
    }
   },
   "source": [
    "embed_size=256 # size of the embeddings\n",
    "num_heads=4 # number of attention heads\n",
    "ff_dim=4 # dimension of the feedforward layer in the encoder\n",
    "num_transformer_blocks=4 # number of encoder blocks\n",
    "mlp_units=[128] # the size of the feedforward layer used to make predictions\n",
    "mlp_dropout=0.4 # dropout in the feedforward layer\n",
    "dropout=0.25 # dropout in the encoder"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "65d227f0-b59c-4be5-be0a-3637718dd61e",
   "metadata": {
    "id": "65d227f0-b59c-4be5-be0a-3637718dd61e",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.190196Z",
     "start_time": "2024-07-09T21:21:14.181950Z"
    }
   },
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N independent but identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "12be91a2-8753-46f4-81e5-ea1472724265",
   "metadata": {
    "id": "12be91a2-8753-46f4-81e5-ea1472724265",
    "outputId": "33a08e16-42b8-4ffe-a916-785fd3627ae6",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.199051Z",
     "start_time": "2024-07-09T21:21:14.192149Z"
    }
   },
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, drop_last = True)\n",
    "len(train_dataloader)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bd06271f-ab4c-444e-b89b-fde0f4a00dbb",
   "metadata": {
    "id": "bd06271f-ab4c-444e-b89b-fde0f4a00dbb",
    "outputId": "d8b7fd9e-def9-45c7-ef55-8bfa938b7941",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.206150Z",
     "start_time": "2024-07-09T21:21:14.200315Z"
    }
   },
   "source": [
    "x,y = next(iter(train_dataloader))\n",
    "xp = x\n",
    "\n",
    "print(xp.shape)\n",
    "print(xp[0][0:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 500, 1])\n",
      "tensor([[2.31860995],\n",
      "        [2.49206209],\n",
      "        [2.32649422]])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "b32544a6-519e-467e-904c-5a4a0880ad1f",
   "metadata": {
    "id": "b32544a6-519e-467e-904c-5a4a0880ad1f",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.213237Z",
     "start_time": "2024-07-09T21:21:14.207154Z"
    }
   },
   "source": [
    "class PtMultiheadAttention(nn.Module):\n",
    "  def __init__(self, head_size, num_heads, dropout=0.1):\n",
    "    super(PtMultiheadAttention, self).__init__()\n",
    "    assert head_size % num_heads == 0\n",
    "\n",
    "    self.d_k = head_size // num_heads\n",
    "    self.weight_matrices = clones(nn.Linear(head_size, head_size), 4)\n",
    "    self.attn = None\n",
    "    if dropout > 0:\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def _attention(self, query, key, value, mask=None, dropout=None):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    # if mask is not None:\n",
    "    #   scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    # if dropout is not None:\n",
    "    #   p_attn = self.dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "  def forward(self, query, key, value):\n",
    "\n",
    "    # get q, k and v\n",
    "    query, key, value = [\n",
    "      weights(inputs)\n",
    "      for weights, inputs in zip(self.weight_matrices, (query, key, value))\n",
    "    ]\n",
    "\n",
    "    # calculate attention\n",
    "    x, self.attn = self._attention(query, key, value)\n",
    "\n",
    "    return self.weight_matrices[-1](x)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "0989880c-00f7-4a4d-87d2-4c8be5630ae3",
   "metadata": {
    "id": "0989880c-00f7-4a4d-87d2-4c8be5630ae3",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:14.741290Z",
     "start_time": "2024-07-09T21:21:14.734471Z"
    }
   },
   "source": [
    "class PytorchEncoder(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0):\n",
    "    super(PytorchEncoder, self).__init__()\n",
    "    # attention\n",
    "    self.embedding = nn.Linear(in_features=inputs.shape[-1], out_features=embed_size)\n",
    "    self.attention = PtMultiheadAttention(embed_size, num_heads, dropout=0.0)\n",
    "    self.linear1 = nn.Linear(embed_size, 1)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = nn.LayerNorm(normalized_shape=inputs.shape[-1], eps=1e-6)\n",
    "\n",
    "    # feedforward\n",
    "    self.conv1 = nn.Conv1d(in_channels=inputs.shape[-1], out_channels=ff_dim, kernel_size=1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=inputs.shape[-1], kernel_size=1)\n",
    "    self.layer_norm2 = nn.LayerNorm(normalized_shape=inputs.shape[1], eps=1e-6)\n",
    "\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.embedding(src)\n",
    "    x = self.attention(x, x, x)[0]\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    res = x + src\n",
    "    res = res.reshape(res.shape[0], res.shape[2], res.shape[1])\n",
    "\n",
    "    x = self.conv1(res)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = x + res\n",
    "\n",
    "    return x.reshape(x.shape[0], x.shape[-1], x.shape[1])"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "425b351b-f492-47da-83fd-c4bc975d20d5",
   "metadata": {
    "id": "425b351b-f492-47da-83fd-c4bc975d20d5",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:15.270719Z",
     "start_time": "2024-07-09T21:21:15.265735Z"
    }
   },
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class EncoderClassifier(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0, num_blocks=4):\n",
    "    super(EncoderClassifier, self).__init__()\n",
    "    encoder_layer = PytorchEncoder(inputs=inputs, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "    encoders = OrderedDict()\n",
    "    for idx in range(num_blocks):\n",
    "      encoders[f\"encoder{idx}\"] = encoder_layer\n",
    "    self.encoder_block = nn.Sequential(encoders)\n",
    "    self.avg = nn.AvgPool1d(kernel_size=1)\n",
    "    self.dense1 = nn.Linear(500, mlp_units[0])\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dense2 = nn.Linear(mlp_units[0], 2)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder_block(x)\n",
    "    x = torch.squeeze(self.avg(x), 2)\n",
    "    x = self.dense1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.softmax(x)\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "fe6ffdfe-cd88-4f34-b1b2-2d0cd012bc38",
   "metadata": {
    "id": "fe6ffdfe-cd88-4f34-b1b2-2d0cd012bc38",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:20.177861Z",
     "start_time": "2024-07-09T21:21:16.636552Z"
    }
   },
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "model = EncoderClassifier(inputs=xp, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "9242bc11-f6a8-4b21-8589-bc0b5de7aee4",
   "metadata": {
    "id": "9242bc11-f6a8-4b21-8589-bc0b5de7aee4",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:21:21.171554Z",
     "start_time": "2024-07-09T21:21:21.165536Z"
    }
   },
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "      inputs, labels = data\n",
    "      optimizer.zero_grad()\n",
    "      if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels.to(torch.long).reshape(-1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(outputs, axis=1)\n",
    "      correct_labels = labels.squeeze()\n",
    "\n",
    "      correct += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "      iterations += 1\n",
    "    last_loss = running_loss / len(train_dataloader)\n",
    "    acc = (correct / iterations)\n",
    "\n",
    "    return last_loss, acc"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "0b5b2801-d11a-41b4-a3c0-d69d626398dc",
   "metadata": {
    "editable": true,
    "tags": [],
    "id": "0b5b2801-d11a-41b4-a3c0-d69d626398dc",
    "outputId": "687ad248-3ae1-48df-88b0-79caab811d0c",
    "ExecuteTime": {
     "end_time": "2024-07-09T21:22:13.241453Z",
     "start_time": "2024-07-09T21:21:22.518437Z"
    }
   },
   "source": [
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss, acc = train_one_epoch(epoch)\n",
    "\n",
    "    print(avg_loss)\n",
    "    print(acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taihim/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7034551245825631\n",
      "tensor(54.07366180, device='cuda:0')\n",
      "EPOCH 2:\n",
      "0.6694063427192825\n",
      "tensor(58.95647430, device='cuda:0')\n",
      "EPOCH 3:\n",
      "0.6425807656986373\n",
      "tensor(63.36495972, device='cuda:0')\n",
      "EPOCH 4:\n",
      "0.6202069999916213\n",
      "tensor(65.95982361, device='cuda:0')\n",
      "EPOCH 5:\n",
      "0.6014440421547208\n",
      "tensor(69.02902222, device='cuda:0')\n",
      "EPOCH 6:\n",
      "0.5879234075546265\n",
      "tensor(71.20536041, device='cuda:0')\n",
      "EPOCH 7:\n",
      "0.5749064194304603\n",
      "tensor(71.90290833, device='cuda:0')\n",
      "EPOCH 8:\n",
      "0.5640652759798935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m avg_loss, acc \u001B[38;5;241m=\u001B[39m train_one_epoch(epoch)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(avg_loss)\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43macc\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/_tensor.py:464\u001B[0m, in \u001B[0;36mTensor.__repr__\u001B[0;34m(self, tensor_contents)\u001B[0m\n\u001B[1;32m    460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    461\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__repr__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, tensor_contents\u001B[38;5;241m=\u001B[39mtensor_contents\n\u001B[1;32m    462\u001B[0m     )\n\u001B[1;32m    463\u001B[0m \u001B[38;5;66;03m# All strings are unicode in Python 3.\u001B[39;00m\n\u001B[0;32m--> 464\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tensor_str\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/_tensor_str.py:697\u001B[0m, in \u001B[0;36m_str\u001B[0;34m(self, tensor_contents)\u001B[0m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad(), torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39m_python_dispatch\u001B[38;5;241m.\u001B[39m_disable_current_modes():\n\u001B[1;32m    696\u001B[0m     guard \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_DisableFuncTorch()\n\u001B[0;32m--> 697\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_str_intern\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtensor_contents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_contents\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/_tensor_str.py:617\u001B[0m, in \u001B[0;36m_str_intern\u001B[0;34m(inp, tensor_contents)\u001B[0m\n\u001B[1;32m    615\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m _tensor_str(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_dense(), indent)\n\u001B[1;32m    616\u001B[0m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 617\u001B[0m                     tensor_str \u001B[38;5;241m=\u001B[39m \u001B[43m_tensor_str\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout \u001B[38;5;241m!=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstrided:\n\u001B[1;32m    620\u001B[0m     suffixes\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayout=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayout))\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/_tensor_str.py:349\u001B[0m, in \u001B[0;36m_tensor_str\u001B[0;34m(self, indent)\u001B[0m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\n\u001B[1;32m    346\u001B[0m         \u001B[38;5;28mself\u001B[39m, indent, summarize, real_formatter, imag_formatter\n\u001B[1;32m    347\u001B[0m     )\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 349\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m \u001B[43m_Formatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_summarized_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msummarize\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    350\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _tensor_str_with_formatter(\u001B[38;5;28mself\u001B[39m, indent, summarize, formatter)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dedc7-02eb-47a9-aca0-18865d2a1194",
   "metadata": {
    "editable": true,
    "tags": [],
    "id": "7c6dedc7-02eb-47a9-aca0-18865d2a1194",
    "outputId": "32c42f0a-1421-48a4-8f9a-5cd9b6f66a8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(79.68750000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "iteration = 0\n",
    "for data in test_dataloader:\n",
    "  iteration += 1\n",
    "  inputs, labels = data\n",
    "  if torch.cuda.is_available():\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  outputs = model(inputs)\n",
    "  predictions = torch.argmax(outputs, axis=1)\n",
    "  correct_labels = labels.squeeze().int()\n",
    "\n",
    "  acc += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "print(acc/iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759e130-26e2-4b2e-a369-1bb064f5104e",
   "metadata": {
    "editable": true,
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8759e130-26e2-4b2e-a369-1bb064f5104e",
    "outputId": "951bb6c0-c17f-4758-cb5c-01446a51e302"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 114MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ResNet18_Weights, resnet18\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "model = resnet18(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGnIE_u3DwYf",
    "outputId": "d0940bda-7023-4eb5-e9c8-3ebda64a0215"
   },
   "id": "XGnIE_u3DwYf",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "type(weights.transforms()(img).unsqueeze(0))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GcbsRDmD2Hr",
    "outputId": "024d6b63-d527-4b0a-c6d7-57e68075ae7f"
   },
   "id": "0GcbsRDmD2Hr",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from PIL.Image import Image\n",
    "from PIL.Image import open as pil_open\n",
    "\n",
    "def load_img_from_path(file_path: Path) -> Image:\n",
    "    \"\"\"Loads an image from a file path.\"\"\"\n",
    "    with open(file_path, \"rb+\") as f:\n",
    "        img = pil_open(f)\n",
    "        # TODO: fix type hinting for load method\n",
    "        img.load()  # type: ignore\n",
    "    return img"
   ],
   "metadata": {
    "id": "8Qvr7u2jD8OL"
   },
   "id": "8Qvr7u2jD8OL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "img = load_img_from_path(Path(\"/content/bird.JPEG\"))"
   ],
   "metadata": {
    "id": "E1m1bMCcEAY-"
   },
   "id": "E1m1bMCcEAY-",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(Path(\"/content/bird.JPEG\"), \"rb\") as f:\n",
    "  print(f)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "9frICE5uELTG",
    "outputId": "6233caff-19d2-43d9-a574-2731cb883093"
   },
   "id": "9frICE5uELTG",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "bad mode 'rb'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-c39e5c7c75cd>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mPath\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"/content/bird.JPEG\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"rb\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3203\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3204\u001B[0m         \u001B[0mmsg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34mf\"bad mode {repr(mode)}\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3205\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3206\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mStringIO\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3207\u001B[0m         msg = (\n",
      "\u001B[0;31mValueError\u001B[0m: bad mode 'rb'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "/content/bird.JPEG"
   ],
   "metadata": {
    "id": "tfmsBOfRERf0"
   },
   "id": "tfmsBOfRERf0",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
