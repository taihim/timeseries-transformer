{
 "cells": [
  {
   "cell_type": "code",
   "id": "2ed092cc-d9f0-40ba-838f-3c96920e8016",
   "metadata": {
    "id": "2ed092cc-d9f0-40ba-838f-3c96920e8016",
    "ExecuteTime": {
     "end_time": "2024-07-10T20:57:53.361105Z",
     "start_time": "2024-07-10T20:57:53.357300Z"
    }
   },
   "source": [
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.set_printoptions(precision=8)\n",
    "torch.manual_seed(32)\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\""
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:25.296771Z",
     "start_time": "2024-07-10T21:46:25.291976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FordDatasetKfold(Dataset):\n",
    "  def __init__(self, sequences, labels):\n",
    "    self.labels = labels\n",
    "    self.sequences = sequences\n",
    "    self.num_classes = len(torch.unique(self.labels)) # count the number of unique labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.sequences.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sequence = torch.reshape(self.sequences[idx], (-1, 1)) # dim: seq_len x num_features\n",
    "    label = torch.reshape(self.labels[idx], (-1, )) # dim: 1 x 1\n",
    "\n",
    "    return sequence, label"
   ],
   "id": "186b82de9175817",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:25.621292Z",
     "start_time": "2024-07-10T21:46:25.567959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FordDataset(Dataset):\n",
    "  def __init__(self, split=\"train\"):\n",
    "    self.root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "    self.data = torch.tensor(np.loadtxt(self.root_url + \"FordA_TRAIN.tsv\", delimiter=\"\\t\"), dtype=torch.float32) if split==\"train\" else torch.tensor(np.loadtxt(self.root_url + \"FordA_TEST.tsv\", delimiter=\"\\t\"), dtype=torch.float32)\n",
    "    self.labels = self.data[:, 0] # get first element from each example\n",
    "    self.sequences = self.data[:, 1:] # get all elements after first element\n",
    "    self.labels[self.labels == -1] = 0 # change all -1 labels to 0\n",
    "    self.num_classes = len(torch.unique(self.labels)) # count the number of unique labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.data.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sequence = torch.reshape(self.sequences[idx], (-1, 1)) # dim: seq_len x num_features\n",
    "    label = torch.reshape(self.labels[idx], (-1, )) # dim: 1 x 1\n",
    "\n",
    "    return sequence, label\n",
    "\n",
    "test_dataset = FordDataset(\"test\")"
   ],
   "id": "5c8c750a40f20fe8",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:25.854973Z",
     "start_time": "2024-07-10T21:46:25.711470Z"
    }
   },
   "cell_type": "code",
   "source": "train_data = torch.tensor(np.loadtxt(root_url + \"FordA_TRAIN.tsv\", delimiter=\"\\t\"), dtype=torch.float32)",
   "id": "c78738ac4fa4228e",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:25.863115Z",
     "start_time": "2024-07-10T21:46:25.859539Z"
    }
   },
   "cell_type": "code",
   "source": "train_data.shape",
   "id": "c531413f0c18debf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3601, 501])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:25.974728Z",
     "start_time": "2024-07-10T21:46:25.972398Z"
    }
   },
   "cell_type": "code",
   "source": "y = train_data[:, 0]",
   "id": "70fdcf04602a5b4a",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.097995Z",
     "start_time": "2024-07-10T21:46:26.094653Z"
    }
   },
   "cell_type": "code",
   "source": "y[y == -1] = 0",
   "id": "e8ceb737578cd085",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.229782Z",
     "start_time": "2024-07-10T21:46:26.227089Z"
    }
   },
   "cell_type": "code",
   "source": "x = train_data[:, 1:] # get all elements after first element",
   "id": "f6db72179b9da51d",
   "outputs": [],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.354832Z",
     "start_time": "2024-07-10T21:46:26.351668Z"
    }
   },
   "cell_type": "code",
   "source": "skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=32)",
   "id": "97c651b723382bd7",
   "outputs": [],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.470929Z",
     "start_time": "2024-07-10T21:46:26.467080Z"
    }
   },
   "cell_type": "code",
   "source": "x.shape",
   "id": "725fed82dcf5f761",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3601, 500])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.586920Z",
     "start_time": "2024-07-10T21:46:26.582564Z"
    }
   },
   "cell_type": "code",
   "source": "y.shape",
   "id": "7e721f1f9c2ddbcc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3601])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.721679Z",
     "start_time": "2024-07-10T21:46:26.719114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_sets = []\n",
    "val_sets = []"
   ],
   "id": "266882bc3156ff59",
   "outputs": [],
   "execution_count": 178
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:26.874898Z",
     "start_time": "2024-07-10T21:46:26.866201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for train_idx, val_idx in skf.split(x, y):\n",
    "    # Create PyTorch DataLoader for training and validation\n",
    "    train_sets.append(FordDatasetKfold(x[train_idx], y[train_idx]))\n",
    "    val_sets.append(FordDatasetKfold(x[val_idx], y[val_idx]))\n",
    "    "
   ],
   "id": "cb5314274d69a8c6",
   "outputs": [],
   "execution_count": 179
  },
  {
   "cell_type": "code",
   "id": "7a96e134-d7c9-4ddc-843a-29b7201e8eac",
   "metadata": {
    "id": "7a96e134-d7c9-4ddc-843a-29b7201e8eac",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:27.619713Z",
     "start_time": "2024-07-10T21:46:27.615449Z"
    }
   },
   "source": "train_sets",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.FordDatasetKfold at 0x7f20ef706e40>,\n",
       " <__main__.FordDatasetKfold at 0x7f20acbbdbb0>]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 180
  },
  {
   "cell_type": "code",
   "id": "60b6dce8-9c83-4c2a-98cc-f2997f1c4f4e",
   "metadata": {
    "id": "60b6dce8-9c83-4c2a-98cc-f2997f1c4f4e",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:29.163719Z",
     "start_time": "2024-07-10T21:46:29.160835Z"
    }
   },
   "source": [
    "embed_size=256 # size of the embeddings\n",
    "num_heads=4 # number of attention heads\n",
    "ff_dim=4 # dimension of the feedforward layer in the encoder\n",
    "num_transformer_blocks=2 # number of encoder blocks\n",
    "mlp_units=[128] # the size of the feedforward layer used to make predictions\n",
    "mlp_dropout=0.4 # dropout in the feedforward layer\n",
    "dropout=0.25 # dropout in the encoder"
   ],
   "outputs": [],
   "execution_count": 181
  },
  {
   "cell_type": "code",
   "id": "65d227f0-b59c-4be5-be0a-3637718dd61e",
   "metadata": {
    "id": "65d227f0-b59c-4be5-be0a-3637718dd61e",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:29.706151Z",
     "start_time": "2024-07-10T21:46:29.703361Z"
    }
   },
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N independent but identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "outputs": [],
   "execution_count": 182
  },
  {
   "cell_type": "code",
   "id": "12be91a2-8753-46f4-81e5-ea1472724265",
   "metadata": {
    "id": "12be91a2-8753-46f4-81e5-ea1472724265",
    "outputId": "33a08e16-42b8-4ffe-a916-785fd3627ae6",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:30.368404Z",
     "start_time": "2024-07-10T21:46:30.362492Z"
    }
   },
   "source": [
    "train_dataloaders = [DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, drop_last = True) for train_dataset in train_sets]\n",
    "val_dataloaders = [DataLoader(dataset=val_dataset, batch_size=64, shuffle=True, drop_last = True) for val_dataset in val_sets]\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, drop_last = True)\n",
    "len(train_dataloaders)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 183
  },
  {
   "cell_type": "code",
   "id": "bd06271f-ab4c-444e-b89b-fde0f4a00dbb",
   "metadata": {
    "id": "bd06271f-ab4c-444e-b89b-fde0f4a00dbb",
    "outputId": "d8b7fd9e-def9-45c7-ef55-8bfa938b7941",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:31.501360Z",
     "start_time": "2024-07-10T21:46:31.495647Z"
    }
   },
   "source": [
    "x,y = next(iter(train_dataloaders[0]))\n",
    "xp = x\n",
    "\n",
    "print(xp.shape)\n",
    "print(xp[0][0:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 500, 1])\n",
      "tensor([[-0.16406339],\n",
      "        [-0.01160359],\n",
      "        [ 0.09336790]])\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "cell_type": "code",
   "id": "b32544a6-519e-467e-904c-5a4a0880ad1f",
   "metadata": {
    "id": "b32544a6-519e-467e-904c-5a4a0880ad1f",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:34.088097Z",
     "start_time": "2024-07-10T21:46:34.081995Z"
    }
   },
   "source": [
    "class PtMultiheadAttention(nn.Module):\n",
    "  def __init__(self, head_size, num_heads, dropout=0.1):\n",
    "    super(PtMultiheadAttention, self).__init__()\n",
    "    assert head_size % num_heads == 0\n",
    "\n",
    "    self.d_k = head_size // num_heads\n",
    "    self.weight_matrices = clones(nn.Linear(head_size, head_size), 4)\n",
    "    self.attn = None\n",
    "    if dropout > 0:\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def _attention(self, query, key, value, mask=None, dropout=None):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    # if mask is not None:\n",
    "    #   scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    # if dropout is not None:\n",
    "    #   p_attn = self.dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "  def forward(self, query, key, value):\n",
    "\n",
    "    # get q, k and v\n",
    "    query, key, value = [\n",
    "      weights(inputs)\n",
    "      for weights, inputs in zip(self.weight_matrices, (query, key, value))\n",
    "    ]\n",
    "\n",
    "    # calculate attention\n",
    "    x, self.attn = self._attention(query, key, value)\n",
    "\n",
    "    return self.weight_matrices[-1](x)"
   ],
   "outputs": [],
   "execution_count": 185
  },
  {
   "cell_type": "code",
   "id": "0989880c-00f7-4a4d-87d2-4c8be5630ae3",
   "metadata": {
    "id": "0989880c-00f7-4a4d-87d2-4c8be5630ae3",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:34.243691Z",
     "start_time": "2024-07-10T21:46:34.236654Z"
    }
   },
   "source": [
    "class PytorchEncoder(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0):\n",
    "    super(PytorchEncoder, self).__init__()\n",
    "    # attention\n",
    "    self.embedding = nn.Linear(in_features=inputs.shape[-1], out_features=embed_size)\n",
    "    self.attention = PtMultiheadAttention(embed_size, num_heads, dropout=0.0)\n",
    "    self.linear1 = nn.Linear(embed_size, 1)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = nn.LayerNorm(normalized_shape=inputs.shape[-1], eps=1e-6)\n",
    "\n",
    "    # feedforward\n",
    "    self.conv1 = nn.Conv1d(in_channels=inputs.shape[-1], out_channels=ff_dim, kernel_size=1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=inputs.shape[-1], kernel_size=1)\n",
    "    self.layer_norm2 = nn.LayerNorm(normalized_shape=inputs.shape[1], eps=1e-6)\n",
    "\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.embedding(src)\n",
    "    x = self.attention(x, x, x)[0]\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    res = x + src\n",
    "    res = res.reshape(res.shape[0], res.shape[2], res.shape[1])\n",
    "\n",
    "    x = self.conv1(res)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = x + res\n",
    "\n",
    "    return x.reshape(x.shape[0], x.shape[-1], x.shape[1])"
   ],
   "outputs": [],
   "execution_count": 186
  },
  {
   "cell_type": "code",
   "id": "425b351b-f492-47da-83fd-c4bc975d20d5",
   "metadata": {
    "id": "425b351b-f492-47da-83fd-c4bc975d20d5",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:59:36.519077Z",
     "start_time": "2024-07-10T21:59:36.513956Z"
    }
   },
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class EncoderClassifier(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0, num_blocks=2):\n",
    "    super(EncoderClassifier, self).__init__()\n",
    "    encoder_layer = PytorchEncoder(inputs=inputs, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "    encoders = OrderedDict()\n",
    "    for idx in range(num_blocks):\n",
    "      encoders[f\"encoder{idx}\"] = encoder_layer\n",
    "    self.encoder_block = nn.Sequential(encoders)\n",
    "    self.avg = nn.AvgPool1d(kernel_size=1)\n",
    "    self.dense1 = nn.Linear(500, mlp_units[0])\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dense2 = nn.Linear(mlp_units[0], 2)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder_block(x)\n",
    "    x = torch.squeeze(self.avg(x), 2)\n",
    "    x = self.dense1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.softmax(x)\n",
    "    return x"
   ],
   "outputs": [],
   "execution_count": 200
  },
  {
   "cell_type": "code",
   "id": "fe6ffdfe-cd88-4f34-b1b2-2d0cd012bc38",
   "metadata": {
    "id": "fe6ffdfe-cd88-4f34-b1b2-2d0cd012bc38",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:35.462213Z",
     "start_time": "2024-07-10T21:46:35.459453Z"
    }
   },
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "# model = EncoderClassifier(inputs=xp, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "# model.cuda()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "execution_count": 188
  },
  {
   "cell_type": "code",
   "id": "9242bc11-f6a8-4b21-8589-bc0b5de7aee4",
   "metadata": {
    "id": "9242bc11-f6a8-4b21-8589-bc0b5de7aee4",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:46:35.883962Z",
     "start_time": "2024-07-10T21:46:35.876633Z"
    }
   },
   "source": [
    "def train_one_epoch(epoch_index, train_data, val_data, model):\n",
    "    train_running_loss = 0.\n",
    "    train_last_loss = 0.\n",
    "    train_correct = 0\n",
    "    iterations = 0\n",
    "    \n",
    "    for i, (inputs,labels) in enumerate(train_data):\n",
    "      if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "          \n",
    "      model[\"opt\"].zero_grad()\n",
    "      outputs = model[\"model\"](inputs)\n",
    "      loss = model[\"loss_fn\"](outputs, labels.to(torch.long).reshape(-1))\n",
    "      loss.backward()\n",
    "      model[\"opt\"].step()\n",
    "      train_running_loss += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(outputs, axis=1)\n",
    "      correct_labels = labels.squeeze()\n",
    "\n",
    "      train_correct += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "      iterations += 1\n",
    "    train_last_loss = train_running_loss / len(train_data)\n",
    "    train_acc = (train_correct / iterations)\n",
    "    \n",
    "    \n",
    "    \n",
    "    val_running_loss = 0.\n",
    "    val_last_loss = 0.\n",
    "    val_correct = 0\n",
    "    \n",
    "    iterations = 0\n",
    "    model[\"model\"].eval()\n",
    "    for i, (inputs,labels) in enumerate(val_data):\n",
    "      if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "      outputs = model[\"model\"](inputs)\n",
    "      loss = model[\"loss_fn\"](outputs, labels.to(torch.long).reshape(-1))\n",
    "      val_running_loss += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(outputs, axis=1)\n",
    "      correct_labels = labels.squeeze()\n",
    "\n",
    "      val_correct += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "      iterations += 1\n",
    "    val_last_loss = val_running_loss / len(val_data)\n",
    "    val_acc = (val_correct / iterations)\n",
    "\n",
    "    return train_last_loss, train_acc, val_last_loss, val_acc"
   ],
   "outputs": [],
   "execution_count": 189
  },
  {
   "cell_type": "code",
   "id": "0b5b2801-d11a-41b4-a3c0-d69d626398dc",
   "metadata": {
    "editable": true,
    "tags": [],
    "id": "0b5b2801-d11a-41b4-a3c0-d69d626398dc",
    "outputId": "687ad248-3ae1-48df-88b0-79caab811d0c",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:58:14.825154Z",
     "start_time": "2024-07-10T21:56:09.406975Z"
    }
   },
   "source": [
    "epochs = 30\n",
    "models = []\n",
    "for fold in range(len(train_dataloaders)):\n",
    "  print('FOLD {}:'.format(fold + 1))\n",
    "  \n",
    "  train_dataloader = train_dataloaders[fold]\n",
    "  val_dataloader = val_dataloaders[fold]\n",
    "  \n",
    "  model = EncoderClassifier(inputs=xp, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "  model.cuda()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  models.append({\"model\": model, \"loss_fn\": criterion, \"opt\": optimizer})\n",
    "  \n",
    "  for epoch in range(epochs):\n",
    "      print('EPOCH {}:'.format(epoch + 1))\n",
    "  \n",
    "      models[fold][\"model\"].train(True)\n",
    "      train_avg_loss, train_acc, val_avg_loss, val_acc = train_one_epoch(epoch, train_dataloader, val_dataloader, models[fold])\n",
    "  \n",
    "      print(train_avg_loss)\n",
    "      print(train_acc)\n",
    "      print(val_avg_loss)\n",
    "      print(val_acc)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1:\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taihim/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7403944943632398\n",
      "tensor(49.66518021, device='cuda:0')\n",
      "0.7202982391629901\n",
      "tensor(52.45536041, device='cuda:0')\n",
      "EPOCH 2:\n",
      "0.6678970690284457\n",
      "tensor(59.15178680, device='cuda:0')\n",
      "0.7028910879577909\n",
      "tensor(55.85937881, device='cuda:0')\n",
      "EPOCH 3:\n",
      "0.6301428462777819\n",
      "tensor(65.73661041, device='cuda:0')\n",
      "0.684229640024049\n",
      "tensor(58.59375381, device='cuda:0')\n",
      "EPOCH 4:\n",
      "0.5951072169201714\n",
      "tensor(69.64286041, device='cuda:0')\n",
      "0.634695542710168\n",
      "tensor(64.34152222, device='cuda:0')\n",
      "EPOCH 5:\n",
      "0.5642683388931411\n",
      "tensor(74.21875000, device='cuda:0')\n",
      "0.625593227999551\n",
      "tensor(66.40625000, device='cuda:0')\n",
      "EPOCH 6:\n",
      "0.5493544489145279\n",
      "tensor(75.11161041, device='cuda:0')\n",
      "0.6044448720557349\n",
      "tensor(68.63839722, device='cuda:0')\n",
      "EPOCH 7:\n",
      "0.5266892037221363\n",
      "tensor(77.28794861, device='cuda:0')\n",
      "0.5980315655469894\n",
      "tensor(68.97322083, device='cuda:0')\n",
      "EPOCH 8:\n",
      "0.5008184143475124\n",
      "tensor(81.30580902, device='cuda:0')\n",
      "0.612719252705574\n",
      "tensor(67.91294861, device='cuda:0')\n",
      "EPOCH 9:\n",
      "0.499453709593841\n",
      "tensor(81.25000000, device='cuda:0')\n",
      "0.5889605645622525\n",
      "tensor(69.97768402, device='cuda:0')\n",
      "EPOCH 10:\n",
      "0.47910886045013157\n",
      "tensor(82.75669861, device='cuda:0')\n",
      "0.5919663576143128\n",
      "tensor(70.20089722, device='cuda:0')\n",
      "EPOCH 11:\n",
      "0.46278430627925055\n",
      "tensor(84.31919861, device='cuda:0')\n",
      "0.5993063620158604\n",
      "tensor(69.64286041, device='cuda:0')\n",
      "EPOCH 12:\n",
      "0.46082114215408054\n",
      "tensor(84.65402222, device='cuda:0')\n",
      "0.5810026026197842\n",
      "tensor(71.93080902, device='cuda:0')\n",
      "EPOCH 13:\n",
      "0.4443603189928191\n",
      "tensor(87.05357361, device='cuda:0')\n",
      "0.573813969535487\n",
      "tensor(71.93080902, device='cuda:0')\n",
      "EPOCH 14:\n",
      "0.4356176406145096\n",
      "tensor(88.05803680, device='cuda:0')\n",
      "0.5751897458519254\n",
      "tensor(72.09822083, device='cuda:0')\n",
      "EPOCH 15:\n",
      "0.4278967423098428\n",
      "tensor(89.06250763, device='cuda:0')\n",
      "0.5718468385083335\n",
      "tensor(72.43303680, device='cuda:0')\n",
      "EPOCH 16:\n",
      "0.41557954783950535\n",
      "tensor(90.23438263, device='cuda:0')\n",
      "0.5733080378600529\n",
      "tensor(72.04241180, device='cuda:0')\n",
      "EPOCH 17:\n",
      "0.4200375463281359\n",
      "tensor(88.89509583, device='cuda:0')\n",
      "0.5741360144955772\n",
      "tensor(71.87500000, device='cuda:0')\n",
      "EPOCH 18:\n",
      "0.41044412021126064\n",
      "tensor(90.56919861, device='cuda:0')\n",
      "0.5740389419453484\n",
      "tensor(72.48884583, device='cuda:0')\n",
      "EPOCH 19:\n",
      "0.40250258786337717\n",
      "tensor(91.62947083, device='cuda:0')\n",
      "0.5635971256664821\n",
      "tensor(73.49330902, device='cuda:0')\n",
      "EPOCH 20:\n",
      "0.39799727286611286\n",
      "tensor(91.79688263, device='cuda:0')\n",
      "0.5622462161949703\n",
      "tensor(73.43750000, device='cuda:0')\n",
      "EPOCH 21:\n",
      "0.3875362298318318\n",
      "tensor(92.96875763, device='cuda:0')\n",
      "0.5604399910994938\n",
      "tensor(73.93973541, device='cuda:0')\n",
      "EPOCH 22:\n",
      "0.38504515162536074\n",
      "tensor(93.13616180, device='cuda:0')\n",
      "0.5681573546358517\n",
      "tensor(72.87947083, device='cuda:0')\n",
      "EPOCH 23:\n",
      "0.3892305088894708\n",
      "tensor(92.29911041, device='cuda:0')\n",
      "0.568575127848557\n",
      "tensor(72.48884583, device='cuda:0')\n",
      "EPOCH 24:\n",
      "0.37901952862739563\n",
      "tensor(93.75000763, device='cuda:0')\n",
      "0.5639264466507095\n",
      "tensor(72.82366180, device='cuda:0')\n",
      "EPOCH 25:\n",
      "0.37233972017254147\n",
      "tensor(94.53125763, device='cuda:0')\n",
      "0.5581874634538379\n",
      "tensor(73.88393402, device='cuda:0')\n",
      "EPOCH 26:\n",
      "0.37604296313864843\n",
      "tensor(93.91741180, device='cuda:0')\n",
      "0.560530920113836\n",
      "tensor(73.32589722, device='cuda:0')\n",
      "EPOCH 27:\n",
      "0.36508405208587646\n",
      "tensor(94.97768402, device='cuda:0')\n",
      "0.5565133786627224\n",
      "tensor(74.27455902, device='cuda:0')\n",
      "EPOCH 28:\n",
      "0.3625660112925938\n",
      "tensor(95.42411041, device='cuda:0')\n",
      "0.5590254557984216\n",
      "tensor(74.38616180, device='cuda:0')\n",
      "EPOCH 29:\n",
      "0.37161339074373245\n",
      "tensor(94.47544861, device='cuda:0')\n",
      "0.5648213816540582\n",
      "tensor(73.54911041, device='cuda:0')\n",
      "EPOCH 30:\n",
      "0.3688929336411612\n",
      "tensor(94.53125763, device='cuda:0')\n",
      "0.5563839301466942\n",
      "tensor(74.10714722, device='cuda:0')\n",
      "FOLD 2:\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 62.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[198], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEPOCH \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     18\u001B[0m models[fold][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 19\u001B[0m train_avg_loss, train_acc, val_avg_loss, val_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfold\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_avg_loss)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_acc)\n",
      "Cell \u001B[0;32mIn[189], line 39\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch_index, train_data, val_data, model)\u001B[0m\n\u001B[1;32m     37\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m     38\u001B[0m   labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m---> 39\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m loss \u001B[38;5;241m=\u001B[39m model[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss_fn\u001B[39m\u001B[38;5;124m\"\u001B[39m](outputs, labels\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mlong)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     41\u001B[0m val_running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[187], line 19\u001B[0m, in \u001B[0;36mEncoderClassifier.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 19\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m   x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mavg(x), \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     21\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense1(x)\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[186], line 21\u001B[0m, in \u001B[0;36mPytorchEncoder.forward\u001B[0;34m(self, src)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, src):\n\u001B[1;32m     20\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(src)\n\u001B[0;32m---> 21\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     22\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear1(x)\n\u001B[1;32m     23\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[185], line 30\u001B[0m, in \u001B[0;36mPtMultiheadAttention.forward\u001B[0;34m(self, query, key, value)\u001B[0m\n\u001B[1;32m     24\u001B[0m query, key, value \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     25\u001B[0m   weights(inputs)\n\u001B[1;32m     26\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m weights, inputs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_matrices, (query, key, value))\n\u001B[1;32m     27\u001B[0m ]\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# calculate attention\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight_matrices[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m](x)\n",
      "Cell \u001B[0;32mIn[185], line 13\u001B[0m, in \u001B[0;36mPtMultiheadAttention._attention\u001B[0;34m(self, query, key, value, mask, dropout)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_attention\u001B[39m(\u001B[38;5;28mself\u001B[39m, query, key, value, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 13\u001B[0m   scores \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatmul\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k)\n\u001B[1;32m     14\u001B[0m   \u001B[38;5;66;03m# if mask is not None:\u001B[39;00m\n\u001B[1;32m     15\u001B[0m   \u001B[38;5;66;03m#   scores = scores.masked_fill(mask == 0, -1e9)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m   p_attn \u001B[38;5;241m=\u001B[39m scores\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 62.00 MiB. GPU "
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "cell_type": "code",
   "id": "7c6dedc7-02eb-47a9-aca0-18865d2a1194",
   "metadata": {
    "editable": true,
    "tags": [],
    "id": "7c6dedc7-02eb-47a9-aca0-18865d2a1194",
    "outputId": "32c42f0a-1421-48a4-8f9a-5cd9b6f66a8c",
    "ExecuteTime": {
     "end_time": "2024-07-10T21:58:22.325364Z",
     "start_time": "2024-07-10T21:58:21.622290Z"
    }
   },
   "source": [
    "acc = 0\n",
    "iteration = 0\n",
    "for data in test_dataloader:\n",
    "  iteration += 1\n",
    "  inputs, labels = data\n",
    "  if torch.cuda.is_available():\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  outputs = models[0][\"model\"](inputs)\n",
    "  predictions = torch.argmax(outputs, axis=1)\n",
    "  correct_labels = labels.squeeze().int()\n",
    "\n",
    "  acc += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "print(acc/iteration)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taihim/PycharmProjects/timeseries-transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.92187500, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3ab64367c9711b92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
