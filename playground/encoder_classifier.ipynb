{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lJ9eEV79SOS"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import layers\n",
    "torch.set_printoptions(precision=8)\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class FordDataset(Dataset):\n",
    "  def __init__(self, split=\"train\"):\n",
    "    self.root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "    self.data = torch.tensor(np.loadtxt(self.root_url + \"FordA_TRAIN.tsv\", delimiter=\"\\t\"), dtype=torch.float32) if split==\"train\" else torch.tensor(np.loadtxt(self.root_url + \"FordA_TEST.tsv\", delimiter=\"\\t\"), dtype=torch.float32)\n",
    "    self.labels = self.data[:, 0] # get first element from each example\n",
    "    self.sequences = self.data[:, 1:] # get all elements after first element\n",
    "    self.labels[self.labels == -1] = 0 # change all -1 labels to 0\n",
    "    self.num_classes = len(torch.unique(self.labels)) # count the number of unique labels\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.data.shape[0]\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sequence = torch.reshape(self.sequences[idx], (-1, 1)) # dim: seq_len x num_features\n",
    "    label = torch.reshape(self.labels[idx], (-1, )) # dim: 1 x 1\n",
    "\n",
    "    return sequence, label\n",
    "\n",
    "train_dataset = FordDataset(\"train\")\n",
    "test_dataset = FordDataset(\"test\")"
   ],
   "metadata": {
    "id": "IQ5qRX2r9blG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "embed_size=256 # size of the embeddings\n",
    "num_heads=4 # number of attention heads\n",
    "ff_dim=4 # dimension of the feedforward layer in the encoder\n",
    "num_transformer_blocks=4 # number of encoder blocks\n",
    "mlp_units=[128] # the size of the feedforward layer used to make predictions\n",
    "mlp_dropout=0.4 # dropout in the feedforward layer\n",
    "dropout=0.25 # dropout in the encoder"
   ],
   "metadata": {
    "id": "gPXAu_vF9u7D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def keras_encoder(inputs, embed_size, num_heads, ff_dim, dropout=0):\n",
    "    # Attention and Normalization\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=embed_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x + res\n",
    "\n"
   ],
   "metadata": {
    "id": "t6wIPfFW90SA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pytorch_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "  embedding = nn.Linear(1, head_size)\n",
    "  x = embedding(inputs)\n",
    "  x = nn.MultiheadAttention(head_size, num_heads, dropout=0.0)(x, x, x)[0]\n",
    "  x = nn.Linear(head_size, 1)(x)\n",
    "  x = nn.Dropout(dropout)(x)\n",
    "  x = nn.LayerNorm(normalized_shape=x.shape[2], eps=1e-6)(x)\n",
    "  res = x + inputs\n",
    "\n",
    "  x = nn.Conv1d(in_channels=x.shape[1], out_channels=ff_dim, kernel_size=1)(res)\n",
    "  x = nn.ReLU()(x)\n",
    "  x = nn.Dropout(dropout)(x)\n",
    "  print(x.shape)\n",
    "  x = nn.Conv1d(in_channels=x.shape[1], out_channels=inputs.shape[-1], kernel_size=1)(x)\n",
    "  x = nn.LayerNorm(normalized_shape=x.shape[2], eps=1e-6)(x)\n",
    "\n",
    "\n",
    "  return x + res"
   ],
   "metadata": {
    "id": "VSHlP6XK94oF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pytorch_encoder2(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "  # input is 500x1 i.e. each element in the sequence has 1 feature. therefore in_features for nn.Linear is 1. we project this single feature to {head_size} dimensions\n",
    "  embedding = nn.Linear(in_features=1, out_features=head_size)\n",
    "  # input shape is (B x 500 x 1) where B is batch_size\n",
    "  x = embedding(inputs) # embedding projects to (B x 500 x 256)\n",
    "  x = nn.MultiheadAttention(embed_dim=head_size, num_heads=num_heads, dropout=0.0)(x, x, x)[0] # keeps (B x 500 x 256)\n",
    "  x = nn.Linear(head_size, 1)(x) # projects back to (B x 500 x 1)\n",
    "  x = nn.Dropout(dropout)(x) # (B x 500 x 1)\n",
    "  x = nn.LayerNorm(normalized_shape=x.shape[2], eps=1e-6)(x) # (B x 500 x 1)\n",
    "  res = x + inputs\n",
    "\n",
    "  # feedforward layer projects to (B x ff_dim x 1)\n",
    "  res = res.reshape(res.shape[0], res.shape[2], res.shape[1]) # (B x 1 x 500)\n",
    "\n",
    "  x = nn.Conv1d(in_channels=res.shape[1], out_channels=ff_dim, kernel_size=1)(res) # (B x ff_dim x 500)\n",
    "  x = nn.ReLU()(x) # (B x ff_dim x 500)\n",
    "  x = nn.Dropout(dropout)(x) # (B x ff_dim x 500)\n",
    "  x = nn.Conv1d(in_channels=x.shape[1], out_channels=inputs.shape[-1], kernel_size=1)(x) # (B x 1 x 500)\n",
    "  x = nn.LayerNorm(normalized_shape=x.shape[2], eps=1e-6)(x) # (B x 1 x 500)\n",
    "  x = x + res\n",
    "  return x.reshape(inputs.shape[0], inputs.shape[1], inputs.shape[2]) # (B x 500 x 1)"
   ],
   "metadata": {
    "id": "9PjjwZLRRQf5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "nnk1S8mpGwqg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# attention"
   ],
   "metadata": {
    "id": "qJ7mRXUIRNTe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ],
   "metadata": {
    "id": "oZYdNYtcGwsz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N independent but identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "metadata": {
    "id": "ZuwCvh1iGxDU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pt_attn = nn.MultiheadAttention(1, 1, dropout=0.0)\n",
    "\n",
    "pt_attn(xp, xp, xp)[0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mh49D9YjGxFd",
    "outputId": "a5cc74d4-5e3b-4c7f-9ecb-b1d60840e175"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xp.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdwqYBEGUz2M",
    "outputId": "3d34633a-7c6e-4250-e51b-b8658ecaf9de"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "xp_transform = nn.Linear(1, 256)(xp)"
   ],
   "metadata": {
    "id": "zp6yiE4EU1Ag"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xp_transform.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p968jxA3U6nW",
    "outputId": "cfa8177b-021a-43d7-ff9c-c22d7a5482df"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([64, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lin1 = nn.Linear(256, 256)"
   ],
   "metadata": {
    "id": "us889TwNIH8m"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lin1(xp_transform).shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88ywYNspIH_F",
    "outputId": "be59bea3-d27c-46f6-d8be-3d1a9bf4fd2b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 205
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layers = clones(lin1, 4)"
   ],
   "metadata": {
    "id": "IzGkLBN_IIBc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layers"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyR40pcYMybU",
    "outputId": "e52a426f-1a91-4744-f266-365d516e073a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-3): 4 x Linear(in_features=256, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query, key, value = [\n",
    "  lin(x)\n",
    "  for lin, x in zip(linear_layers, (xp_transform, xp_transform, xp_transform))\n",
    "]"
   ],
   "metadata": {
    "id": "jTUdlDy_Myf0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbInjWuZMyiT",
    "outputId": "151ce57a-ae37-444e-d0df-8b6dc8e7f92e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "key.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2Q5uefLPYDU",
    "outputId": "61ac8eaf-7320-4a06-ccf3-55eb232bf062"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 210
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "value.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scW263HSPYFz",
    "outputId": "4bcd0d25-4e87-47a5-b95e-9ae73b4feb76"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "  d_k = query.size(-1)\n",
    "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "      scores = scores.masked_fill(mask == 0, -1e9)\n",
    "  p_attn = scores.softmax(dim=-1)\n",
    "  if dropout is not None:\n",
    "      p_attn = dropout(p_attn)\n",
    "  return torch.matmul(p_attn, value), p_attn"
   ],
   "metadata": {
    "id": "Z7Hshr-pRTHF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "att = attention(query, key, value)"
   ],
   "metadata": {
    "id": "Gem7WwK8RTJf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "att[0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMA8ql4KRTLm",
    "outputId": "dee3b77b-fbe4-4a9f-a98e-16fd3509bca0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 214
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "final_layer = nn.Linear(4, 1)"
   ],
   "metadata": {
    "id": "8wys2tt2RTNz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layers[-1](att[0]).shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6p2WgjhRTQA",
    "outputId": "190b44ca-b318-4535-bb74-71f7460a4e9c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 256])"
      ]
     },
     "metadata": {},
     "execution_count": 215
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "q4o9TylcRTSc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tJFjlWutSeT-",
    "outputId": "35bc0d83-6ceb-48fb-9c59-7a8fe853e34e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 145
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "key.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCoksU73RTUX",
    "outputId": "ab20e79c-57aa-4c18-8e69-0c185b018104"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "key.transpose(-2, -1).shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQIB4d7jSUYz",
    "outputId": "42804a76-9235-4e96-b578-3962abc77bf9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 500])"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(key.shape[-1])\n",
    "scores.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cj6u-CH1Sqa9",
    "outputId": "76fde802-7e67-4ef7-932d-bdc79a70e566"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 500])"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "scores = scores.softmax(-1)"
   ],
   "metadata": {
    "id": "qTPIn5KvSqc3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "att = torch.matmul(scores, value)"
   ],
   "metadata": {
    "id": "1xHqJQqDTjlc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "att.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KS_855eaTn7q",
    "outputId": "a8c231df-4c90-4caf-c638-ae1ef9b87e3a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "scores[0][0:5]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKoZkkNySqhB",
    "outputId": "94d6005b-0470-46f3-998a-f5b76ab48566"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.13987494, 0.16932230, 0.24462357,  ..., 0.71016347, 0.70146340,\n",
       "         0.66761613],\n",
       "        [0.16932230, 0.20496909, 0.29612327,  ..., 0.85967153, 0.84913993,\n",
       "         0.80816686],\n",
       "        [0.24462357, 0.29612327, 0.42781571,  ..., 1.24198604, 1.22677076,\n",
       "         1.16757619],\n",
       "        [0.36374766, 0.44032609, 0.63614863,  ..., 1.84679461, 1.82416999,\n",
       "         1.73614943],\n",
       "        [0.51167548, 0.61939669, 0.89485574,  ..., 2.59784389, 2.56601810,\n",
       "         2.44220161]], grad_fn=<SliceBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 152
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "KAWOSwOnSqjG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# remaining code"
   ],
   "metadata": {
    "id": "R8hAs4JtRK7q"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8UCSTxSYPbIk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8_tPTDS1PbM3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "cBhlY2UcPbPS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "atNu8CO9PbRe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PtMultiheadAttention(nn.Module):\n",
    "  def __init__(self, head_size, num_heads, dropout=0.1):\n",
    "    super(PtMultiheadAttention, self).__init__()\n",
    "    assert head_size % num_heads == 0\n",
    "\n",
    "    self.d_k = head_size // num_heads\n",
    "    self.weight_matrices = clones(nn.Linear(head_size, head_size), 4)\n",
    "    self.attn = None\n",
    "    if dropout > 0:\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def _attention(self, query, key, value, mask=None, dropout=None):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "    # if mask is not None:\n",
    "    #   scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    # if dropout is not None:\n",
    "    #   p_attn = self.dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "  def forward(self, query, key, value):\n",
    "\n",
    "    # get q, k and v\n",
    "    query, key, value = [\n",
    "      weights(inputs)\n",
    "      for weights, inputs in zip(self.weight_matrices, (query, key, value))\n",
    "    ]\n",
    "\n",
    "    # calculate attention\n",
    "    x, self.attn = self._attention(query, key, value)\n",
    "\n",
    "    return self.weight_matrices[-1](x)\n"
   ],
   "metadata": {
    "id": "FxMw3UYrGxHs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m_att = PtMultiheadAttention(256, 4)"
   ],
   "metadata": {
    "id": "SVIMZ5AqXUh3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "m_att(xp_transform).shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "n9dMtrdoXUkR",
    "outputId": "49af5a7a-2ff6-4a2f-b6fe-f7f2887c8f8f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "PtMultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-67-9cb2d3596342>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mm_att\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxp_transform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1532\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1533\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1539\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: PtMultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Bjvd5A1lXUmm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PytorchEncoder(nn.Module):\n",
    "  def __init__(self, inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    super(PytorchEncoder, self).__init__()\n",
    "    # attention\n",
    "    self.embedding = nn.Linear(in_features=inputs.shape[-1], out_features=head_size)\n",
    "    self.attention = nn.MultiheadAttention(head_size, num_heads, dropout=0.0)\n",
    "    self.linear1 = nn.Linear(head_size, 1)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = nn.LayerNorm(normalized_shape=inputs.shape[-1], eps=1e-6)\n",
    "\n",
    "    # feedforward\n",
    "    self.conv1 = nn.Conv1d(in_channels=inputs.shape[1], out_channels=ff_dim, kernel_size=1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=inputs.shape[-1], kernel_size=1)\n",
    "    self.layer_norm2 = nn.LayerNorm(normalized_shape=inputs.shape[2], eps=1e-6)\n",
    "\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.embedding(src)\n",
    "    x = self.attention(x, x, x)[0]\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    res = x + src\n",
    "\n",
    "    # res = res.reshape(res.shape[0], res.shape[2], res.shape[1])\n",
    "    x = self.conv1(res)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.layer_norm2(x)\n",
    "\n",
    "    return x + res\n"
   ],
   "metadata": {
    "id": "PAfWBpTkQg3C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PytorchEncoder2(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0):\n",
    "    super(PytorchEncoder2, self).__init__()\n",
    "    # attention\n",
    "    self.embedding = nn.Linear(in_features=inputs.shape[-1], out_features=embed_size)\n",
    "    self.attention = nn.MultiheadAttention(embed_size, num_heads, dropout=0.0)\n",
    "    self.linear1 = nn.Linear(embed_size, 1)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = nn.LayerNorm(normalized_shape=inputs.shape[-1], eps=1e-6)\n",
    "\n",
    "    # feedforward\n",
    "    self.conv1 = nn.Conv1d(in_channels=inputs.shape[-1], out_channels=ff_dim, kernel_size=1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=inputs.shape[-1], kernel_size=1)\n",
    "    self.layer_norm2 = nn.LayerNorm(normalized_shape=inputs.shape[1], eps=1e-6)\n",
    "\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.embedding(src)\n",
    "    x = self.attention(x, x, x)[0]\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    res = x + src\n",
    "    res = res.reshape(res.shape[0], res.shape[2], res.shape[1])\n",
    "\n",
    "    x = self.conv1(res)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = x + res\n",
    "\n",
    "    return x.reshape(x.shape[0], x.shape[-1], x.shape[1])\n"
   ],
   "metadata": {
    "id": "Plcwz3FHcOXl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class PytorchEncoder3(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0):\n",
    "    super(PytorchEncoder3, self).__init__()\n",
    "    # attention\n",
    "    self.embedding = nn.Linear(in_features=inputs.shape[-1], out_features=embed_size)\n",
    "    self.attention = PtMultiheadAttention(embed_size, num_heads, dropout=0.0)\n",
    "    self.linear1 = nn.Linear(embed_size, 1)\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.layer_norm1 = nn.LayerNorm(normalized_shape=inputs.shape[-1], eps=1e-6)\n",
    "\n",
    "    # feedforward\n",
    "    self.conv1 = nn.Conv1d(in_channels=inputs.shape[-1], out_channels=ff_dim, kernel_size=1)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout2 = nn.Dropout(dropout)\n",
    "    self.conv2 = nn.Conv1d(in_channels=ff_dim, out_channels=inputs.shape[-1], kernel_size=1)\n",
    "    self.layer_norm2 = nn.LayerNorm(normalized_shape=inputs.shape[1], eps=1e-6)\n",
    "\n",
    "\n",
    "  def forward(self, src):\n",
    "    x = self.embedding(src)\n",
    "    x = self.attention(x, x, x)[0]\n",
    "    x = self.linear1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.layer_norm1(x)\n",
    "\n",
    "    res = x + src\n",
    "    res = res.reshape(res.shape[0], res.shape[2], res.shape[1])\n",
    "\n",
    "    x = self.conv1(res)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout2(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.layer_norm2(x)\n",
    "    x = x + res\n",
    "\n",
    "    return x.reshape(x.shape[0], x.shape[-1], x.shape[1])"
   ],
   "metadata": {
    "id": "tPa-FWUfXkTn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "liqEKfvWXjIh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "g = pytorch_encoder(xp, head_size, num_heads, ff_dim, dropout)"
   ],
   "metadata": {
    "id": "SGj2ZyDiVa3W"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "h = keras_encoder(xt, head_size, num_heads, ff_dim, dropout)"
   ],
   "metadata": {
    "id": "mlHSEM-0MeX0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, drop_last = True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, drop_last = True)"
   ],
   "metadata": {
    "id": "aayxb58g97JC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x,y = next(iter(train_dataloader))\n",
    "xp = x\n",
    "xt = np.array(x)\n",
    "\n",
    "print(xt.shape)\n",
    "print(xp.shape)\n",
    "\n",
    "print(xt[0][0:3])\n",
    "print(xp[0][0:3])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIwQ0McU-DHN",
    "outputId": "cac42645-1927-489b-953b-ef3ac4d7f42e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 500, 1)\n",
      "torch.Size([64, 500, 1])\n",
      "[[-0.8186042]\n",
      " [-0.6492901]\n",
      " [-0.4733622]]\n",
      "tensor([[-0.81860417],\n",
      "        [-0.64929008],\n",
      "        [-0.47336221]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class EncoderClassifier(nn.Module):\n",
    "  def __init__(self, inputs, embed_size, num_heads, ff_dim, dropout=0, num_blocks=4):\n",
    "    super(EncoderClassifier, self).__init__()\n",
    "    encoder_layer = PytorchEncoder2(inputs=inputs, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "    encoders = OrderedDict()\n",
    "    for idx in range(num_blocks):\n",
    "      encoders[f\"encoder{idx}\"] = encoder_layer\n",
    "    self.encoder_block = nn.Sequential(encoders)\n",
    "    self.avg = nn.AvgPool1d(kernel_size=1)\n",
    "    self.dense1 = nn.Linear(500, mlp_units[0])\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.dropout1 = nn.Dropout(dropout)\n",
    "    self.dense2 = nn.Linear(mlp_units[0], 2)\n",
    "    self.softmax = nn.Softmax()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder_block(x)\n",
    "    x = torch.squeeze(self.avg(x), 2)\n",
    "    x = self.dense1(x)\n",
    "    x = self.relu1(x)\n",
    "    x = self.dropout1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.softmax(x)\n",
    "    return x"
   ],
   "metadata": {
    "id": "iC3aIKZyWUnC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "model = EncoderClassifier(inputs=xp, embed_size=embed_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout)\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "metadata": {
    "id": "2-C52UFMZWCV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(train_dataloader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzA6xSvLfOYG",
    "outputId": "65126d45-fb41-481e-f8eb-4733cb52df5b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train_one_epoch(epoch_index, model, criterion, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "      inputs, labels = data\n",
    "      optimizer.zero_grad()\n",
    "      if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "      outputs = model(inputs)\n",
    "      loss = criterion(outputs, labels.to(torch.long).reshape(-1))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(outputs, axis=1)\n",
    "      correct_labels = labels.squeeze()\n",
    "\n",
    "      correct += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "      iterations += 1\n",
    "    last_loss = running_loss / len(train_dataloader)\n",
    "    acc = (correct / iterations)\n",
    "\n",
    "    return last_loss, acc"
   ],
   "metadata": {
    "id": "r6y4v80aZLBe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "id": "_Ap0tB8dqJPF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/encoder_trainer_{}'.format(timestamp))"
   ],
   "metadata": {
    "id": "yGmc6abxqwJ5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss, acc = train_one_epoch(epoch, model, criterion, optimizer)\n",
    "\n",
    "    print(avg_loss)\n",
    "    print(acc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YDc2cFJZLDt",
    "outputId": "66593460-a7aa-4e20-8a16-744e959d8c20"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH 1:\n",
      "0.7078477931874139\n",
      "tensor(52.81808090, device='cuda:0')\n",
      "EPOCH 2:\n",
      "0.6663750793252673\n",
      "tensor(59.20759201, device='cuda:0')\n",
      "EPOCH 3:\n",
      "0.6341418836797986\n",
      "tensor(64.53683472, device='cuda:0')\n",
      "EPOCH 4:\n",
      "0.6114352707351957\n",
      "tensor(68.33147430, device='cuda:0')\n",
      "EPOCH 5:\n",
      "0.5948727205395699\n",
      "tensor(69.89397430, device='cuda:0')\n",
      "EPOCH 6:\n",
      "0.5821993468063218\n",
      "tensor(71.54018402, device='cuda:0')\n",
      "EPOCH 7:\n",
      "0.5763263787542071\n",
      "tensor(72.57254791, device='cuda:0')\n",
      "EPOCH 8:\n",
      "0.5652851579444749\n",
      "tensor(73.29799652, device='cuda:0')\n",
      "EPOCH 9:\n",
      "0.5583260027425629\n",
      "tensor(74.24665833, device='cuda:0')\n",
      "EPOCH 10:\n",
      "0.5512227517153535\n",
      "tensor(75.47433472, device='cuda:0')\n",
      "EPOCH 11:\n",
      "0.5429055956857545\n",
      "tensor(76.14397430, device='cuda:0')\n",
      "EPOCH 12:\n",
      "0.5331535908792701\n",
      "tensor(77.95759583, device='cuda:0')\n",
      "EPOCH 13:\n",
      "0.5274207363171237\n",
      "tensor(79.07366180, device='cuda:0')\n",
      "EPOCH 14:\n",
      "0.5248082838952541\n",
      "tensor(78.57143402, device='cuda:0')\n",
      "EPOCH 15:\n",
      "0.52053167032344\n",
      "tensor(79.15737152, device='cuda:0')\n",
      "EPOCH 16:\n",
      "0.5165332459977695\n",
      "tensor(79.71540833, device='cuda:0')\n",
      "EPOCH 17:\n",
      "0.5130329797310489\n",
      "tensor(80.27343750, device='cuda:0')\n",
      "EPOCH 18:\n",
      "0.5102129163486617\n",
      "tensor(80.58036041, device='cuda:0')\n",
      "EPOCH 19:\n",
      "0.5064506748957294\n",
      "tensor(81.22209930, device='cuda:0')\n",
      "EPOCH 20:\n",
      "0.5013431289366314\n",
      "tensor(81.30580902, device='cuda:0')\n",
      "EPOCH 21:\n",
      "0.4981279256088393\n",
      "tensor(82.11495972, device='cuda:0')\n",
      "EPOCH 22:\n",
      "0.49084477179816793\n",
      "tensor(82.42187500, device='cuda:0')\n",
      "EPOCH 23:\n",
      "0.48549176805785726\n",
      "tensor(83.62165833, device='cuda:0')\n",
      "EPOCH 24:\n",
      "0.48604690709284376\n",
      "tensor(83.64955902, device='cuda:0')\n",
      "EPOCH 25:\n",
      "0.4830359523849828\n",
      "tensor(83.78906250, device='cuda:0')\n",
      "EPOCH 26:\n",
      "0.47882273367473055\n",
      "tensor(84.17968750, device='cuda:0')\n",
      "EPOCH 27:\n",
      "0.47493034494774683\n",
      "tensor(84.76562500, device='cuda:0')\n",
      "EPOCH 28:\n",
      "0.4752122552267143\n",
      "tensor(84.23549652, device='cuda:0')\n",
      "EPOCH 29:\n",
      "0.46912095536078724\n",
      "tensor(84.40290833, device='cuda:0')\n",
      "EPOCH 30:\n",
      "0.46752240721668514\n",
      "tensor(85.18415833, device='cuda:0')\n",
      "EPOCH 31:\n",
      "0.4608007302241666\n",
      "tensor(85.79799652, device='cuda:0')\n",
      "EPOCH 32:\n",
      "0.45772848437939373\n",
      "tensor(86.30022430, device='cuda:0')\n",
      "EPOCH 33:\n",
      "0.4575009250215122\n",
      "tensor(86.49553680, device='cuda:0')\n",
      "EPOCH 34:\n",
      "0.45289396388190134\n",
      "tensor(87.13728333, device='cuda:0')\n",
      "EPOCH 35:\n",
      "0.4524670211332185\n",
      "tensor(87.08147430, device='cuda:0')\n",
      "EPOCH 36:\n",
      "0.4491735409413065\n",
      "tensor(87.63951111, device='cuda:0')\n",
      "EPOCH 37:\n",
      "0.4479479023388454\n",
      "tensor(87.97433472, device='cuda:0')\n",
      "EPOCH 38:\n",
      "0.4449673497251102\n",
      "tensor(87.52790833, device='cuda:0')\n",
      "EPOCH 39:\n",
      "0.44082166786704746\n",
      "tensor(88.22544861, device='cuda:0')\n",
      "EPOCH 40:\n",
      "0.44060163998178076\n",
      "tensor(88.16964722, device='cuda:0')\n",
      "EPOCH 41:\n",
      "0.4351562825696809\n",
      "tensor(88.95089722, device='cuda:0')\n",
      "EPOCH 42:\n",
      "0.43513714894652367\n",
      "tensor(89.09040833, device='cuda:0')\n",
      "EPOCH 43:\n",
      "0.4341177951012339\n",
      "tensor(89.09040833, device='cuda:0')\n",
      "EPOCH 44:\n",
      "0.4355344554143293\n",
      "tensor(88.81138611, device='cuda:0')\n",
      "EPOCH 45:\n",
      "0.42929324081965853\n",
      "tensor(89.28572083, device='cuda:0')\n",
      "EPOCH 46:\n",
      "0.42689778975078035\n",
      "tensor(89.64844513, device='cuda:0')\n",
      "EPOCH 47:\n",
      "0.4289755954274109\n",
      "tensor(89.59263611, device='cuda:0')\n",
      "EPOCH 48:\n",
      "0.42412611097097397\n",
      "tensor(90.20647430, device='cuda:0')\n",
      "EPOCH 49:\n",
      "0.42581063296113697\n",
      "tensor(89.95536041, device='cuda:0')\n",
      "EPOCH 50:\n",
      "0.4205557698650019\n",
      "tensor(90.37388611, device='cuda:0')\n",
      "EPOCH 51:\n",
      "0.42377077309148653\n",
      "tensor(89.89955902, device='cuda:0')\n",
      "EPOCH 52:\n",
      "0.4200482389756611\n",
      "tensor(90.23438263, device='cuda:0')\n",
      "EPOCH 53:\n",
      "0.4193574291254793\n",
      "tensor(90.17857361, device='cuda:0')\n",
      "EPOCH 54:\n",
      "0.41558807130370823\n",
      "tensor(90.48549652, device='cuda:0')\n",
      "EPOCH 55:\n",
      "0.41212918502943857\n",
      "tensor(91.01563263, device='cuda:0')\n",
      "EPOCH 56:\n",
      "0.4116049494062151\n",
      "tensor(91.65737152, device='cuda:0')\n",
      "EPOCH 57:\n",
      "0.4142539011580603\n",
      "tensor(91.15513611, device='cuda:0')\n",
      "EPOCH 58:\n",
      "0.41354858555964064\n",
      "tensor(91.21094513, device='cuda:0')\n",
      "EPOCH 59:\n",
      "0.40963965175407274\n",
      "tensor(91.57366180, device='cuda:0')\n",
      "EPOCH 60:\n",
      "0.40753103899104254\n",
      "tensor(91.79688263, device='cuda:0')\n",
      "EPOCH 61:\n",
      "0.40529253227370127\n",
      "tensor(91.74107361, device='cuda:0')\n",
      "EPOCH 62:\n",
      "0.4088000032518591\n",
      "tensor(91.15513611, device='cuda:0')\n",
      "EPOCH 63:\n",
      "0.40429144884858814\n",
      "tensor(91.93638611, device='cuda:0')\n",
      "EPOCH 64:\n",
      "0.4049762540629932\n",
      "tensor(91.74107361, device='cuda:0')\n",
      "EPOCH 65:\n",
      "0.40713275011096683\n",
      "tensor(91.09933472, device='cuda:0')\n",
      "EPOCH 66:\n",
      "0.4015545898250171\n",
      "tensor(92.15959930, device='cuda:0')\n",
      "EPOCH 67:\n",
      "0.3991485419017928\n",
      "tensor(92.35491180, device='cuda:0')\n",
      "EPOCH 68:\n",
      "0.3972189463675022\n",
      "tensor(92.38282013, device='cuda:0')\n",
      "EPOCH 69:\n",
      "0.3982686714402267\n",
      "tensor(92.35491180, device='cuda:0')\n",
      "EPOCH 70:\n",
      "0.39408986802612034\n",
      "tensor(93.19197083, device='cuda:0')\n",
      "EPOCH 71:\n",
      "0.39385989361575674\n",
      "tensor(93.08036041, device='cuda:0')\n",
      "EPOCH 72:\n",
      "0.3930158918457372\n",
      "tensor(93.19197083, device='cuda:0')\n",
      "EPOCH 73:\n",
      "0.3899323636932032\n",
      "tensor(93.47098541, device='cuda:0')\n",
      "EPOCH 74:\n",
      "0.38967649904744966\n",
      "tensor(93.58259583, device='cuda:0')\n",
      "EPOCH 75:\n",
      "0.3920063174196652\n",
      "tensor(93.35938263, device='cuda:0')\n",
      "EPOCH 76:\n",
      "0.3925106163535799\n",
      "tensor(93.02455902, device='cuda:0')\n",
      "EPOCH 77:\n",
      "0.3913691480244909\n",
      "tensor(93.27567291, device='cuda:0')\n",
      "EPOCH 78:\n",
      "0.38825007528066635\n",
      "tensor(93.72209930, device='cuda:0')\n",
      "EPOCH 79:\n",
      "0.3873101568647793\n",
      "tensor(93.72209930, device='cuda:0')\n",
      "EPOCH 80:\n",
      "0.3858163633516857\n",
      "tensor(93.75000763, device='cuda:0')\n",
      "EPOCH 81:\n",
      "0.38339720985719133\n",
      "tensor(94.16853333, device='cuda:0')\n",
      "EPOCH 82:\n",
      "0.38947140159351484\n",
      "tensor(93.10826111, device='cuda:0')\n",
      "EPOCH 83:\n",
      "0.38692711825881687\n",
      "tensor(93.75000763, device='cuda:0')\n",
      "EPOCH 84:\n",
      "0.3843974199678217\n",
      "tensor(93.91741180, device='cuda:0')\n",
      "EPOCH 85:\n",
      "0.3863205079521452\n",
      "tensor(93.38728333, device='cuda:0')\n",
      "EPOCH 86:\n",
      "0.3795973190239498\n",
      "tensor(94.28013611, device='cuda:0')\n",
      "EPOCH 87:\n",
      "0.3806185855397156\n",
      "tensor(94.33594513, device='cuda:0')\n",
      "EPOCH 88:\n",
      "0.37903971703989164\n",
      "tensor(94.33594513, device='cuda:0')\n",
      "EPOCH 89:\n",
      "0.3835085944405624\n",
      "tensor(93.91741180, device='cuda:0')\n",
      "EPOCH 90:\n",
      "0.3767476741756712\n",
      "tensor(94.92188263, device='cuda:0')\n",
      "EPOCH 91:\n",
      "0.3792380083884512\n",
      "tensor(94.19643402, device='cuda:0')\n",
      "EPOCH 92:\n",
      "0.37738459929823875\n",
      "tensor(94.47544861, device='cuda:0')\n",
      "EPOCH 93:\n",
      "0.3756153093917029\n",
      "tensor(95.14509583, device='cuda:0')\n",
      "EPOCH 94:\n",
      "0.3753820625799043\n",
      "tensor(94.47544861, device='cuda:0')\n",
      "EPOCH 95:\n",
      "0.37612055082406315\n",
      "tensor(94.86607361, device='cuda:0')\n",
      "EPOCH 96:\n",
      "0.37726917171052526\n",
      "tensor(94.47544861, device='cuda:0')\n",
      "EPOCH 97:\n",
      "0.3736754134297371\n",
      "tensor(95.03348541, device='cuda:0')\n",
      "EPOCH 98:\n",
      "0.37620136088558603\n",
      "tensor(94.53125763, device='cuda:0')\n",
      "EPOCH 99:\n",
      "0.3722070652459349\n",
      "tensor(95.36830902, device='cuda:0')\n",
      "EPOCH 100:\n",
      "0.3714140547173364\n",
      "tensor(95.22879791, device='cuda:0')\n",
      "EPOCH 101:\n",
      "0.37386641438518253\n",
      "tensor(94.83817291, device='cuda:0')\n",
      "EPOCH 102:\n",
      "0.3687916605600289\n",
      "tensor(95.45201111, device='cuda:0')\n",
      "EPOCH 103:\n",
      "0.3712296842464379\n",
      "tensor(94.97768402, device='cuda:0')\n",
      "EPOCH 104:\n",
      "0.3726863632244723\n",
      "tensor(95.00558472, device='cuda:0')\n",
      "EPOCH 105:\n",
      "0.3717024214565754\n",
      "tensor(94.75447083, device='cuda:0')\n",
      "EPOCH 106:\n",
      "0.3695998947535242\n",
      "tensor(95.31250763, device='cuda:0')\n",
      "EPOCH 107:\n",
      "0.3657571087990488\n",
      "tensor(95.87053680, device='cuda:0')\n",
      "EPOCH 108:\n",
      "0.36345851474574636\n",
      "tensor(95.89844513, device='cuda:0')\n",
      "EPOCH 109:\n",
      "0.36654601458992275\n",
      "tensor(95.53572083, device='cuda:0')\n",
      "EPOCH 110:\n",
      "0.36997555249503683\n",
      "tensor(95.00558472, device='cuda:0')\n",
      "EPOCH 111:\n",
      "0.3646900627229895\n",
      "tensor(95.78683472, device='cuda:0')\n",
      "EPOCH 112:\n",
      "0.3632786827428\n",
      "tensor(95.92634583, device='cuda:0')\n",
      "EPOCH 113:\n",
      "0.3646369581776006\n",
      "tensor(95.84263611, device='cuda:0')\n",
      "EPOCH 114:\n",
      "0.36442346604807035\n",
      "tensor(95.56362152, device='cuda:0')\n",
      "EPOCH 115:\n",
      "0.3622296378016472\n",
      "tensor(95.92634583, device='cuda:0')\n",
      "EPOCH 116:\n",
      "0.3655490513358797\n",
      "tensor(95.78683472, device='cuda:0')\n",
      "EPOCH 117:\n",
      "0.36231944018176626\n",
      "tensor(95.87053680, device='cuda:0')\n",
      "EPOCH 118:\n",
      "0.3638643372271742\n",
      "tensor(95.53572083, device='cuda:0')\n",
      "EPOCH 119:\n",
      "0.3620446441429002\n",
      "tensor(96.06584930, device='cuda:0')\n",
      "EPOCH 120:\n",
      "0.3597257637551853\n",
      "tensor(96.20536041, device='cuda:0')\n",
      "EPOCH 121:\n",
      "0.3616665303707123\n",
      "tensor(95.87053680, device='cuda:0')\n",
      "EPOCH 122:\n",
      "0.36328006110021044\n",
      "tensor(96.01004791, device='cuda:0')\n",
      "EPOCH 123:\n",
      "0.3641338369676045\n",
      "tensor(95.64732361, device='cuda:0')\n",
      "EPOCH 124:\n",
      "0.36297607528311865\n",
      "tensor(95.75893402, device='cuda:0')\n",
      "EPOCH 125:\n",
      "0.3634260722569057\n",
      "tensor(95.75893402, device='cuda:0')\n",
      "EPOCH 126:\n",
      "0.36246721020766665\n",
      "tensor(95.56362152, device='cuda:0')\n",
      "EPOCH 127:\n",
      "0.3597019761800766\n",
      "tensor(96.09375763, device='cuda:0')\n",
      "EPOCH 128:\n",
      "0.36120389880878584\n",
      "tensor(95.56362152, device='cuda:0')\n",
      "EPOCH 129:\n",
      "0.359638119914702\n",
      "tensor(96.09375763, device='cuda:0')\n",
      "EPOCH 130:\n",
      "0.3569931095199926\n",
      "tensor(96.37277222, device='cuda:0')\n",
      "EPOCH 131:\n",
      "0.3571864685841969\n",
      "tensor(96.42857361, device='cuda:0')\n",
      "EPOCH 132:\n",
      "0.35543606749602724\n",
      "tensor(96.62388611, device='cuda:0')\n",
      "EPOCH 133:\n",
      "0.3580337422234671\n",
      "tensor(96.09375763, device='cuda:0')\n",
      "EPOCH 134:\n",
      "0.3554085578237261\n",
      "tensor(96.48438263, device='cuda:0')\n",
      "EPOCH 135:\n",
      "0.35520896475229946\n",
      "tensor(96.51228333, device='cuda:0')\n",
      "EPOCH 136:\n",
      "0.35374072194099426\n",
      "tensor(96.65178680, device='cuda:0')\n",
      "EPOCH 137:\n",
      "0.3537876590022019\n",
      "tensor(96.51228333, device='cuda:0')\n",
      "EPOCH 138:\n",
      "0.3562335233603205\n",
      "tensor(96.26116180, device='cuda:0')\n",
      "EPOCH 139:\n",
      "0.35497009275215013\n",
      "tensor(96.76339722, device='cuda:0')\n",
      "EPOCH 140:\n",
      "0.35665263927408625\n",
      "tensor(96.20536041, device='cuda:0')\n",
      "EPOCH 141:\n",
      "0.3538430349103042\n",
      "tensor(96.73549652, device='cuda:0')\n",
      "EPOCH 142:\n",
      "0.3539557728384222\n",
      "tensor(96.54018402, device='cuda:0')\n",
      "EPOCH 143:\n",
      "0.34998734880770954\n",
      "tensor(97.01451111, device='cuda:0')\n",
      "EPOCH 144:\n",
      "0.35593071420277866\n",
      "tensor(96.28907013, device='cuda:0')\n",
      "EPOCH 145:\n",
      "0.35544475221208166\n",
      "tensor(96.14955902, device='cuda:0')\n",
      "EPOCH 146:\n",
      "0.35064023626702173\n",
      "tensor(96.98661041, device='cuda:0')\n",
      "EPOCH 147:\n",
      "0.3528864357088293\n",
      "tensor(96.90290833, device='cuda:0')\n",
      "EPOCH 148:\n",
      "0.35317254651870045\n",
      "tensor(96.62388611, device='cuda:0')\n",
      "EPOCH 149:\n",
      "0.34958809560963083\n",
      "tensor(97.04241180, device='cuda:0')\n",
      "EPOCH 150:\n",
      "0.3492008075118065\n",
      "tensor(97.12612152, device='cuda:0')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "acc = 0\n",
    "iteration = 0\n",
    "for data in test_dataloader:\n",
    "  iteration += 1\n",
    "  inputs, labels = data\n",
    "  if torch.cuda.is_available():\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "  outputs = model(inputs)\n",
    "  predictions = torch.argmax(outputs, axis=1)\n",
    "  correct_labels = labels.squeeze().int()\n",
    "\n",
    "  acc += (predictions == correct_labels).int().sum()/len(labels) * 100\n",
    "print(acc/iteration)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qxUZPoysX9Y",
    "outputId": "f932c6e2-db9b-44d8-f086-f7b5ce2c5ea8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(77.57812500, device='cuda:0')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "inputs, labels\n",
    "if torch.cuda.is_available():\n",
    "  inputs = inputs.cuda()\n",
    "  labels = labels.cuda()\n",
    "outputs = model(inputs, 4)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_RyqC3umJDA",
    "outputId": "cfc2b0dd-730f-412a-d56a-09c175285645"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "hWyte6YdmSH8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "inputs.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vI6VwOYmg8x",
    "outputId": "000ee16e-02d9-4ab7-d654-5568bad65908"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 500, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 182
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "labels.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2XWbfXTNmjRi",
    "outputId": "d3108f93-e0bd-4ce0-879f-b8303216c596"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "outputs[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZrtYHZomsBF",
    "outputId": "ecaf7479-a7d2-4782-8901-71e5fc5bb403"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.70790344, 0.29209656], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "labels[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi5DKEk6msdF",
    "outputId": "486bf3d7-a705-4f08-c86f-3d3a38f8dc26"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 143
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ztSLSm75msfT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOXebfSVmshm",
    "outputId": "86a99f19-28c3-4894-a60b-76bc0894e10a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 187
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "labels = labels.squeeze()"
   ],
   "metadata": {
    "id": "rcGD5CTmmsjx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "(predictions == labels).int().sum()/len(labels) * 100"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y2GFSA_cmslx",
    "outputId": "1c2a1609-e8a1-4b75-a864-2bd954324781"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(56.25000000, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 190
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "22/32"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYRUKom6msny",
    "outputId": "d27a6254-9cd8-4199-d5df-0800e4b839b3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "try1 = EncoderClassifier(inputs=xp, embed_size=embed_size, num_heads=4, ff_dim=ff_dim, dropout=dropout, num_blocks=4)"
   ],
   "metadata": {
    "id": "xMTLEdgMYvQr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "try1.cuda()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL6jL-GRi-9e",
    "outputId": "fa14b9bc-ddc6-4f7d-a79d-9e7eb848db7b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EncoderClassifier(\n",
       "  (encoder_block): Sequential(\n",
       "    (encoder0): PytorchEncoder2(\n",
       "      (embedding): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (dropout1): Dropout(p=0.25, inplace=False)\n",
       "      (layer_norm1): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "      (conv1): Conv1d(1, 4, kernel_size=(1,), stride=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      (conv2): Conv1d(4, 1, kernel_size=(1,), stride=(1,))\n",
       "      (layer_norm2): LayerNorm((500,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder1): PytorchEncoder2(\n",
       "      (embedding): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (dropout1): Dropout(p=0.25, inplace=False)\n",
       "      (layer_norm1): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "      (conv1): Conv1d(1, 4, kernel_size=(1,), stride=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      (conv2): Conv1d(4, 1, kernel_size=(1,), stride=(1,))\n",
       "      (layer_norm2): LayerNorm((500,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder2): PytorchEncoder2(\n",
       "      (embedding): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (dropout1): Dropout(p=0.25, inplace=False)\n",
       "      (layer_norm1): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "      (conv1): Conv1d(1, 4, kernel_size=(1,), stride=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      (conv2): Conv1d(4, 1, kernel_size=(1,), stride=(1,))\n",
       "      (layer_norm2): LayerNorm((500,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (encoder3): PytorchEncoder2(\n",
       "      (embedding): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (dropout1): Dropout(p=0.25, inplace=False)\n",
       "      (layer_norm1): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "      (conv1): Conv1d(1, 4, kernel_size=(1,), stride=(1,))\n",
       "      (relu1): ReLU()\n",
       "      (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      (conv2): Conv1d(4, 1, kernel_size=(1,), stride=(1,))\n",
       "      (layer_norm2): LayerNorm((500,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avg): AvgPool1d(kernel_size=(1,), stride=(1,), padding=(0,))\n",
       "  (dense1): Linear(in_features=500, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dense2): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "try1.forward(xp)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "7LtTJyxuY5pB",
    "outputId": "e0124bd6-d003-4df8-8c7a-5862a3d3e1b8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-56-6c652eac7f0d>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtry1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxp\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-41-55d5fafb49ef>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder_block\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mavg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdense1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1532\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1533\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1539\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    216\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 217\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    218\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1532\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1533\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1539\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-f621e0117d15>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, src)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mattention\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear1\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compiled_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[misc]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1531\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1532\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1533\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1534\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_call_impl\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1539\u001B[0m                 \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_pre_hooks\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0m_global_backward_hooks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torchsummaryX==1.1.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLCOrwAekmcl",
    "outputId": "74d66979-764a-4e63-8b5a-6a3c008e94c4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting torchsummaryX==1.1.0\n",
      "  Downloading torchsummaryX-1.1.0-py3-none-any.whl (2.9 kB)\n",
      "Installing collected packages: torchsummaryX\n",
      "Successfully installed torchsummaryX-1.1.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummaryX import summary\n",
    "summary(try1, torch.zeros((1, 500, 1)).cuda())"
   ],
   "metadata": {
    "id": "LEj6b8Axkmen"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for _ in range(num_transformer_blocks):\n",
    "  xp = pt_model(xp)\n",
    "xp = torch.squeeze(nn.AvgPool1d(kernel_size=1)(xp), 2)\n",
    "\n",
    "for dim in mlp_units:\n",
    "  pt_dense1 = nn.Linear(xp.shape[-1], dim)\n",
    "  xp = pt_dense1(xp)\n",
    "  xp = nn.ReLU()(xp)\n",
    "  # xp = nn.Dropout(mlp_dropout)(xp)\n",
    "pt_dense2 = nn.Linear(mlp_units[0], 2)\n",
    "pytorch_outputs = pt_dense2(xp)\n",
    "pytorch_outputs = nn.Softmax()(pytorch_outputs)"
   ],
   "metadata": {
    "id": "1OhLLxvX-Jb6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pt_weights1 = pt_dense1.weight.detach().numpy().transpose()\n",
    "pt_bias1 = pt_dense1.bias.detach().numpy()\n",
    "\n",
    "pt_weights2 = pt_dense2.weight.detach().numpy().transpose()\n",
    "pt_bias2 = pt_dense2.bias.detach().numpy()\n",
    "\n",
    "\n",
    "for _ in range(num_transformer_blocks):\n",
    "  xt = keras_encoder(xt, head_size, num_heads, ff_dim, dropout)\n",
    "xt = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(xt)\n",
    "for dim in mlp_units:\n",
    "  keras_dense = layers.Dense(dim, activation='relu')\n",
    "  keras_dense(xt)\n",
    "  keras_dense.set_weights([pt_weights1, pt_bias1])\n",
    "  xt = keras_dense(xt)\n",
    "  # xt = layers.Dropout(mlp_dropout)(xt)\n",
    "keras_dense2 = layers.Dense(2, activation='softmax')\n",
    "keras_dense2(xt)\n",
    "keras_dense2.set_weights([pt_weights2, pt_bias2])\n",
    "keras_outputs = keras_dense2(xt)"
   ],
   "metadata": {
    "id": "c--AwEA1-LNt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(xt[0][0:5])\n",
    "print(xp[0][0:5])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aO2OPrt-L6g",
    "outputId": "b40b823e-8e05-43c9-ebcc-c012b032b070"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0.        1.2949885 0.        0.        0.       ], shape=(5,), dtype=float32)\n",
      "tensor([0.00000000, 1.29498851, 0.00000000, 0.00000000, 0.00000000],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(keras_outputs)\n",
    "print(pytorch_outputs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4LB6qgtF-N1w",
    "outputId": "6a1aba70-fb18-49d0-b087-57e3a47df7f7"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[0.46704075 0.53295934]], shape=(1, 2), dtype=float32)\n",
      "tensor([[0.46704066, 0.53295940]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_loss = keras.losses.sparse_categorical_crossentropy(y, keras_outputs)\n",
    "print(keras_loss)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "km6BmrG--Q1-",
    "outputId": "31ecf600-0f5b-4b2a-9658-18612df1f6e3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0.6293102], shape=(1,), dtype=float32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Euv968eAHmk",
    "outputId": "3a45a326-ce94-4553-83e9-91bf6d1b7d01"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 397
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "pt_loss = nn.CrossEntropyLoss()(pytorch_outputs, y.to(torch.long).reshape(-1))"
   ],
   "metadata": {
    "id": "kZv2OGkI--d7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pt_loss"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UuvNno9hBqlv",
    "outputId": "e98d665a-6026-4f88-e9e4-ef48f1c1506a"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.66073090, grad_fn=<NllLossBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 399
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# pytorch training"
   ],
   "metadata": {
    "id": "WIqSni9hHVIN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class pytorch_model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(pytorch_model, self).__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.encoder(x)"
   ],
   "metadata": {
    "id": "kKP5Ns3YHN3C"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PIkDA-J1HXVa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "9qeZOZmxHXXu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "9dKaOYE-HXbs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fraM9y7bHXeI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# keras training"
   ],
   "metadata": {
    "id": "GPv0yqlbHRXZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename, delimiter=\"\\t\")\n",
    "    y = data[:, 0]\n",
    "    x = data[:, 1:]\n",
    "    return x, y.astype(int)\n",
    "\n",
    "\n",
    "root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n",
    "\n",
    "x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n",
    "x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "idx = np.random.permutation(len(x_train))\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx]\n",
    "\n",
    "y_train[y_train == -1] = 0\n",
    "y_test[y_test == -1] = 0"
   ],
   "metadata": {
    "id": "_99VKkXnE4BU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = keras_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n"
   ],
   "metadata": {
    "id": "n4ysjHM3E4UU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_train.shape[1:]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppKnUvDgFMh6",
    "outputId": "4cbad88f-8a5f-4f2d-a4e7-e49dd70df78d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3601, 500, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYPqRwmdFS7u",
    "outputId": "11512fd6-e99b-4d20-a9d1-9567361c5142"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3601,)"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150,\n",
    "    batch_size=1,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model.evaluate(x_test, y_test, verbose=1)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WRlFh8a8E7Kh",
    "outputId": "42fbe8c9-e7fd-4b3a-da95-58829c4280d5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 500, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " multi_head_attention_22 (M  (None, 500, 1)               7169      ['input_1[0][0]',             \n",
      " ultiHeadAttention)                                                  'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)        (None, 500, 1)               0         ['multi_head_attention_22[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_40 (La  (None, 500, 1)               2         ['dropout_65[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 500, 1)               0         ['layer_normalization_40[0][0]\n",
      " Lambda)                                                            ',                            \n",
      "                                                                     'input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)          (None, 500, 4)               8         ['tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)        (None, 500, 4)               0         ['conv1d_40[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)          (None, 500, 1)               5         ['dropout_66[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_41 (La  (None, 500, 1)               2         ['conv1d_41[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 500, 1)               0         ['layer_normalization_41[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_23 (M  (None, 500, 1)               7169      ['tf.__operators__.add_1[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)        (None, 500, 1)               0         ['multi_head_attention_23[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_42 (La  (None, 500, 1)               2         ['dropout_67[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 500, 1)               0         ['layer_normalization_42[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)          (None, 500, 4)               8         ['tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)        (None, 500, 4)               0         ['conv1d_42[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)          (None, 500, 1)               5         ['dropout_68[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_43 (La  (None, 500, 1)               2         ['conv1d_43[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 500, 1)               0         ['layer_normalization_43[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_24 (M  (None, 500, 1)               7169      ['tf.__operators__.add_3[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)        (None, 500, 1)               0         ['multi_head_attention_24[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_44 (La  (None, 500, 1)               2         ['dropout_69[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, 500, 1)               0         ['layer_normalization_44[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)          (None, 500, 4)               8         ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)        (None, 500, 4)               0         ['conv1d_44[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_45 (Conv1D)          (None, 500, 1)               5         ['dropout_70[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_45 (La  (None, 500, 1)               2         ['conv1d_45[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TF  (None, 500, 1)               0         ['layer_normalization_45[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " multi_head_attention_25 (M  (None, 500, 1)               7169      ['tf.__operators__.add_5[0][0]\n",
      " ultiHeadAttention)                                                 ',                            \n",
      "                                                                     'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_71 (Dropout)        (None, 500, 1)               0         ['multi_head_attention_25[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_46 (La  (None, 500, 1)               2         ['dropout_71[0][0]']          \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TF  (None, 500, 1)               0         ['layer_normalization_46[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_5[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_46 (Conv1D)          (None, 500, 4)               8         ['tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dropout_72 (Dropout)        (None, 500, 4)               0         ['conv1d_46[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_47 (Conv1D)          (None, 500, 1)               5         ['dropout_72[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_47 (La  (None, 500, 1)               2         ['conv1d_47[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TF  (None, 500, 1)               0         ['layer_normalization_47[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'tf.__operators__.add_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5  (None, 1)                    0         ['tf.__operators__.add_7[0][0]\n",
      "  (GlobalAveragePooling1D)                                          ']                            \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 128)                  256       ['global_average_pooling1d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_73 (Dropout)        (None, 128)                  0         ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 2)                    258       ['dropout_73[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29258 (114.29 KB)\n",
      "Trainable params: 29258 (114.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-144-4cc0d009b671>\u001B[0m in \u001B[0;36m<cell line: 23>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mEarlyStopping\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpatience\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrestore_best_weights\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m model.fit(\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1805\u001B[0m                         ):\n\u001B[1;32m   1806\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1807\u001B[0;31m                             \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1808\u001B[0m                             \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1809\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    831\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 832\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    833\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    834\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    903\u001B[0m         \u001B[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0;31m# no_variable_creation function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 905\u001B[0;31m         return tracing_compilation.call_function(\n\u001B[0m\u001B[1;32m    906\u001B[0m             \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_no_variable_creation_config\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m         )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001B[0m in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m   \u001B[0mbound_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    138\u001B[0m   \u001B[0mflat_inputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack_inputs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbound_args\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 139\u001B[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m    140\u001B[0m       \u001B[0mflat_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m   )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1321\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1322\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1323\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_inference_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_preflattened\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1324\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1325\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001B[0m in \u001B[0;36mcall_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mcall_preflattened\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mSequence\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m     \u001B[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 216\u001B[0;31m     \u001B[0mflat_outputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    217\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction_type\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpack_output\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflat_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001B[0m in \u001B[0;36mcall_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mrecord\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_recording\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m           \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bound_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m             outputs = self._bound_context.call_function(\n\u001B[0m\u001B[1;32m    252\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m                 \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36mcall_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1484\u001B[0m     \u001B[0mcancellation_context\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcancellation\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1485\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcancellation_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1486\u001B[0;31m       outputs = execute.execute(\n\u001B[0m\u001B[1;32m   1487\u001B[0m           \u001B[0mname\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"utf-8\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1488\u001B[0m           \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnum_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 53\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     54\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     55\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_model_outputs = model.predict(sample_x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNCgr9qeE-Pl",
    "outputId": "c250e8ad-f7f6-4b42-e5a5-12549fd91940"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 435ms/step\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_model_outputs.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUtpyYNjH38P",
    "outputId": "8e65a999-b543-417c-cdc9-babca6a8dca3"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 165
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "keras_model_outputs"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rvSYtQLIoWg",
    "outputId": "3d05df33-3bcd-4758-e0d8-fd25b67fc1ce"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.49983075, 0.5001692 ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 166
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "R58G_Vq8Izhs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
